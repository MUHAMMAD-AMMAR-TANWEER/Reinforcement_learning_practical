{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Install Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gym #for building environments\n",
    "from stable_baselines3 import PPO#Selected a one basic model for it \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv#mostly open Ai gym allows you to train on multiple environment this is the wrapper function\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #for Evaluation of Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Ð’ox :n dimensional tensor, range of values\n",
    "E.g. Box (0,1,shape=(3,3))\n",
    "Discrete :- set of items\n",
    "E.g. Discrete (3)\n",
    "Tuple - tuple of other spaces e.g.Box or Discrete\n",
    "E.g. Tuple ( (Discrete (2), Box (0,100, shape= (1,))))\n",
    "Dict - dictionary of spaces e.g. Box or Discrete\n",
    "E.g. Dict: ({'height':Discrete (2), \"speed\":Box (0,100, shape= (1, )) })\n",
    "MultiBinary one hot encoded binary values\n",
    "E.g. MultiBinary(4)\n",
    "MultiDiscrete - multiple discrete value\n",
    "Eg. MultiDiscrete([2,4,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CartPole-v0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 35.0\n",
      "Episode: 2 Score: 11.0\n",
      "Episode: 3 Score: 17.0\n",
      "Episode: 4 Score: 12.0\n",
      "Episode: 5 Score: 46.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()#Initial observations\n",
    "    done = False\n",
    "    score =0\n",
    "\n",
    "    while not done:\n",
    "        env.render()#view environment\n",
    "        action = env.action_space.sample()#pick action from action space\n",
    "        n_states,reward,done,info = env.step(action)#apply action to the environment \n",
    "        score +=reward\n",
    "    print('Episode: {} Score: {}'.format(episode,score))\n",
    "env.close()#close environment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Episode is like a full game the limit of some models for 1 episode is fixed like 200 frames for CartPole but some has this varibale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Understanding Environmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.action_space# this tells action type\n",
    "# env.action_space.sample()#it gives (0 or 1 ) because we have 2 discrete values zero and one action zero left action 1 right   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.4787750e+00, -2.7301290e+37, -1.2654702e-01, -1.6757597e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space\n",
    "env.observation_space.sample()\n",
    "#1 cart position cart velocity pole angle pole angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01098211,  0.01933999,  0.02342481,  0.01444309])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01059531,  0.2141183 ,  0.02371367, -0.27075796]), 1.0, False, {})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory first\n",
    "log_path = os.path.join('Training','Logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])#wrapped our environment \n",
    "model = PPO('MlpPolicy',env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 298  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009219332 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00669    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.05        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 451         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007903871 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 40.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008083684 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 48.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087868385 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.608       |\n",
      "|    explained_variance   | 0.318        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.6         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0141      |\n",
      "|    value_loss           | 63.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005305552 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 69.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005993811 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00871    |\n",
      "|    value_loss           | 56.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 569         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003627795 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.28        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00491    |\n",
      "|    value_loss           | 46.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 579          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050245365 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0059      |\n",
      "|    value_loss           | 34.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 589          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048913457 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.58        |\n",
      "|    explained_variance   | 0.666        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.6         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00495     |\n",
      "|    value_loss           | 61.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1dc95ffc788>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_PATH = os.path.join('Training','Logs','PPO_MODEL_MY_CARTPOLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "#load saved model\n",
    "model = PPO.load(PPO_PATH , env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)#return avearage and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 200.0\n",
      "Episode: 2 Score: 200.0\n",
      "Episode: 3 Score: 200.0\n",
      "Episode: 4 Score: 200.0\n",
      "Episode: 5 Score: 200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()#Initial observations\n",
    "    done = False\n",
    "    score =0\n",
    "\n",
    "    while not done:\n",
    "        env.render()#view environment\n",
    "        action, _ = model.predict(obs) #we use model to do cartpole now\n",
    "        obs,reward,done,info = env.step(action)#apply action to the environment \n",
    "        score +=reward\n",
    "    print('Episode: {} Score: {}'.format(episode,score))\n",
    "env.close()#close environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 score comes because reward for one action is is 1 (when pole not fall) and 0 when fall so the return or score is sum of rewards\n",
    "#These are model less environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Viewing logs on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Adding Callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training','Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callbacks = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env,callback_on_new_best=stop_callbacks,eval_freq=10000, best_model_save_path=save_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy',env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_2\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | 21.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 203      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26          |\n",
      "|    ep_rew_mean          | 26          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 313         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008237669 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000714    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5           |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 48.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.8        |\n",
      "|    ep_rew_mean          | 33.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010153286 |\n",
      "|    clip_fraction        | 0.0714      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 44.3        |\n",
      "|    ep_rew_mean          | 44.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 432         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008911734 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.631      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 49.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=170.80 +/- 24.59\n",
      "Episode length: 170.80 +/- 24.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 171          |\n",
      "|    mean_reward          | 171          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071648173 |\n",
      "|    clip_fraction        | 0.0675       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.303        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    value_loss           | 58.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 57.2     |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 450      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 73.5         |\n",
      "|    ep_rew_mean          | 73.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062807417 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.457        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 59.9         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 90.6       |\n",
      "|    ep_rew_mean          | 90.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00829482 |\n",
      "|    clip_fraction        | 0.067      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.579     |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.65       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00996   |\n",
      "|    value_loss           | 57.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | 106          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899867 |\n",
      "|    clip_fraction        | 0.072        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.59         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00854     |\n",
      "|    value_loss           | 60           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | 122         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005457162 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 54.9        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051772194 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.0998       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.8         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1541be3f808>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Changing the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | 21       |\n",
      "| time/              |          |\n",
      "|    fps             | 686      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | 29.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014992548 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.00113     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.81        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 18.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 44.2       |\n",
      "|    ep_rew_mean          | 44.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 505        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01705216 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.649     |\n",
      "|    explained_variance   | 0.424      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.44       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 26.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 60          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010669075 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012616446 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77       |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 468      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 92.5       |\n",
      "|    ep_rew_mean          | 92.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00915642 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.556     |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.62       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    value_loss           | 42.5       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 109          |\n",
      "|    ep_rew_mean          | 109          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043391427 |\n",
      "|    clip_fraction        | 0.0426       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.267        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.5         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 127          |\n",
      "|    ep_rew_mean          | 127          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053802906 |\n",
      "|    clip_fraction        | 0.0739       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.539       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 49.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    value_loss           | 70.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 144          |\n",
      "|    ep_rew_mean          | 144          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048103062 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.528       |\n",
      "|    explained_variance   | 0.486        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71           |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 87.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034099177 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.522       |\n",
      "|    explained_variance   | 0.484        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.3         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000853    |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    fps             | 460      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x15421a5ab08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 Using Alternate model beside PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\A2C_1\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 30.1     |\n",
      "|    ep_rew_mean        | 30.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 544      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.669   |\n",
      "|    explained_variance | 0.115    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.88     |\n",
      "|    value_loss         | 9.16     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 45.2     |\n",
      "|    ep_rew_mean        | 45.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 566      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.655   |\n",
      "|    explained_variance | 0.00606  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -11.1    |\n",
      "|    value_loss         | 197      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 46.5     |\n",
      "|    ep_rew_mean        | 46.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 517      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | 0.0271   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.68     |\n",
      "|    value_loss         | 6.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 48.3     |\n",
      "|    ep_rew_mean        | 48.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.666   |\n",
      "|    explained_variance | 0.00397  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 5.63     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 53.2     |\n",
      "|    ep_rew_mean        | 53.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 468      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.579   |\n",
      "|    explained_variance | -0.0305  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.63     |\n",
      "|    value_loss         | 5.3      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 56.6     |\n",
      "|    ep_rew_mean        | 56.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | -0.00302 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.807    |\n",
      "|    value_loss         | 4.44     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 62.3     |\n",
      "|    ep_rew_mean        | 62.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.551   |\n",
      "|    explained_variance | 0.000793 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 3.91     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 64       |\n",
      "|    ep_rew_mean        | 64       |\n",
      "| time/                 |          |\n",
      "|    fps                | 470      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.585   |\n",
      "|    explained_variance | 0.0027   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.34     |\n",
      "|    value_loss         | 3.4      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.5     |\n",
      "|    ep_rew_mean        | 67.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 477      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.638   |\n",
      "|    explained_variance | 0.000215 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.761    |\n",
      "|    value_loss         | 2.9      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 71.9     |\n",
      "|    ep_rew_mean        | 71.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 478      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.588   |\n",
      "|    explained_variance | 0.000142 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.926    |\n",
      "|    value_loss         | 2.45     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 76.3     |\n",
      "|    ep_rew_mean        | 76.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.578   |\n",
      "|    explained_variance | 0.000124 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -10      |\n",
      "|    value_loss         | 616      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 79.6     |\n",
      "|    ep_rew_mean        | 79.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 482      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.613   |\n",
      "|    explained_variance | 0.00037  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.614    |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 84.1     |\n",
      "|    ep_rew_mean        | 84.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 479      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.592   |\n",
      "|    explained_variance | 1.98e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.537    |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 87.1      |\n",
      "|    ep_rew_mean        | 87.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.448    |\n",
      "|    explained_variance | -3.59e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.612     |\n",
      "|    value_loss         | 0.998     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 90.7     |\n",
      "|    ep_rew_mean        | 90.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.458   |\n",
      "|    explained_variance | 4.16e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.486    |\n",
      "|    value_loss         | 0.739    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 93.3     |\n",
      "|    ep_rew_mean        | 93.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 480      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0.000142 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.261    |\n",
      "|    value_loss         | 0.514    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 96.1     |\n",
      "|    ep_rew_mean        | 96.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 479      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | 3.55e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.366    |\n",
      "|    value_loss         | 0.333    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 98.3     |\n",
      "|    ep_rew_mean        | 98.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 485      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.593   |\n",
      "|    explained_variance | -2.3e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.136    |\n",
      "|    value_loss         | 0.194    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 99.5     |\n",
      "|    ep_rew_mean        | 99.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 488      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | 4.77e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.119    |\n",
      "|    value_loss         | 0.0916   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 99.4      |\n",
      "|    ep_rew_mean        | 99.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 492       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.386    |\n",
      "|    explained_variance | -6.04e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.135     |\n",
      "|    value_loss         | 0.0283    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 104       |\n",
      "|    ep_rew_mean        | 104       |\n",
      "| time/                 |           |\n",
      "|    fps                | 494       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.446    |\n",
      "|    explained_variance | -0.000825 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 0.0203    |\n",
      "|    value_loss         | 0.00138   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 107      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.483   |\n",
      "|    explained_variance | -0.00604 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.00173  |\n",
      "|    value_loss         | 1.8e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 490      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.539   |\n",
      "|    explained_variance | 2.59e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.000663 |\n",
      "|    value_loss         | 2.01e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 114      |\n",
      "|    ep_rew_mean        | 114      |\n",
      "| time/                 |          |\n",
      "|    fps                | 491      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.44    |\n",
      "|    explained_variance | -0.00498 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.00231  |\n",
      "|    value_loss         | 1.68e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 116      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 486      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.566   |\n",
      "|    explained_variance | 2.35e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.000242 |\n",
      "|    value_loss         | 5.54e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 119      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 478      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.604   |\n",
      "|    explained_variance | 0.000698 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 9.8e-05  |\n",
      "|    value_loss         | 7.52e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 123       |\n",
      "|    ep_rew_mean        | 123       |\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.412    |\n",
      "|    explained_variance | -0.000115 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 0.00116   |\n",
      "|    value_loss         | 2.18e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 126      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 478      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.598   |\n",
      "|    explained_variance | -0.00156 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.00111  |\n",
      "|    value_loss         | 1.04e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 129       |\n",
      "|    ep_rew_mean        | 129       |\n",
      "| time/                 |           |\n",
      "|    fps                | 479       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.49     |\n",
      "|    explained_variance | -1.67e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -25.1     |\n",
      "|    value_loss         | 3.88e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 133      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 482      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | -0.00811 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00316  |\n",
      "|    value_loss         | 2.57e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 135      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 485      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.56    |\n",
      "|    explained_variance | 0.000593 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 5.81e-05 |\n",
      "|    value_loss         | 2.24e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 139       |\n",
      "|    ep_rew_mean        | 139       |\n",
      "| time/                 |           |\n",
      "|    fps                | 488       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.539    |\n",
      "|    explained_variance | -0.000538 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 0.000224  |\n",
      "|    value_loss         | 3.94e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 491      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.596   |\n",
      "|    explained_variance | 8.34e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.00183  |\n",
      "|    value_loss         | 1.81e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 145       |\n",
      "|    ep_rew_mean        | 145       |\n",
      "| time/                 |           |\n",
      "|    fps                | 494       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.496    |\n",
      "|    explained_variance | -1.31e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -27.2     |\n",
      "|    value_loss         | 3.88e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 148      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 497      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.603   |\n",
      "|    explained_variance | 2.98e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.000689 |\n",
      "|    value_loss         | 4.26e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 146      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 500      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.343   |\n",
      "|    explained_variance | 0.0239   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.000432 |\n",
      "|    value_loss         | 1.73e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 149      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 502      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.385   |\n",
      "|    explained_variance | -0.00607 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00137  |\n",
      "|    value_loss         | 2.91e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 151       |\n",
      "|    ep_rew_mean        | 151       |\n",
      "| time/                 |           |\n",
      "|    fps                | 505       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.406    |\n",
      "|    explained_variance | -1.67e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -4.21     |\n",
      "|    value_loss         | 3.88e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 152      |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | 3.16e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.000745 |\n",
      "|    value_loss         | 4.07e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 150      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | 7.27e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.00214  |\n",
      "|    value_loss         | 1.63e-05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x15421a97388>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60eb1d0f96393a1e0420bb6ca0f5435a81d9c3ff4319ce3c90a222065ab89d85"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
